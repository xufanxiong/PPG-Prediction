{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xufanxiong/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import AveragePooling1D, Activation, Flatten, Dense\n",
    "from keras.layers import GlobalMaxPooling1D, Dropout\n",
    "from keras.layers import GlobalAveragePooling1D, UpSampling1D\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD, Adam, Nadam, Adamax\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras.backend as K\n",
    "from data_in_2s import data_in, normali\n",
    "from ori_model import ori_model\n",
    "#from tqdm import tqdm\n",
    "#from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = 6\n",
    "X_train, y_train = data_in(num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train is: (3225, 1000, 5)\n",
      "Shape of y_train is: (3225, 1)\n"
     ]
    }
   ],
   "source": [
    "#Nomalization\n",
    "#y_train = (y_train-50)/150\n",
    "#X_train = normali(X_train)\n",
    "print('Shape of X_train is: ' + str(X_train.shape))\n",
    "print('Shape of y_train is: ' + str(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_test is: (299, 1000, 5)\n",
      "The shape of y_test is: (299, 1)\n"
     ]
    }
   ],
   "source": [
    "#Import and Split test data\n",
    "\n",
    "name_x = 'Training_data/DATA_%.2d_TYPE02.mat'%num_test\n",
    "in_tem_x = sio.loadmat(name_x)['sig']\n",
    "in_tem_x = in_tem_x.T\n",
    "\n",
    "name_y = 'Training_data/DATA_%.2d_TYPE02_BPMtrace.mat'%num_test\n",
    "in_tem_y = sio.loadmat(name_y)['BPM0']\n",
    "\n",
    "X_test_tem = np.zeros((in_tem_y.shape[0]*2, 125*8, 5))\n",
    "\n",
    "k = 0\n",
    "\n",
    "for j in range(in_tem_y.shape[0]*2-1):\n",
    "\tX_test_tem[j, :, :] =  in_tem_x[k:(k+125*8), 1:]\n",
    "\tk += 125\n",
    "\n",
    "tem_y = np.zeros((in_tem_y.shape[0]*2, 1))\n",
    "index = [a*2 for a in range(in_tem_y.shape[0])]\n",
    "tem_y[index] = in_tem_y\n",
    "\n",
    "for b in range(tem_y.shape[0]-1):\n",
    "\tif tem_y[b] == 0:\n",
    "\t\ttem_y[b] = (tem_y[b-1] + tem_y[b+1])/2.\n",
    "\n",
    "y_test = tem_y[:-1, :]\n",
    "X_test = X_test_tem[:-1, :, :]\n",
    "\n",
    "#y_test = (y_test-50)/150\n",
    "#X_test = normali(X_test)\n",
    "\n",
    "print('The shape of X_test is: ' + str(X_test.shape))\n",
    "print('The shape of y_test is: ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of y_train_onehot is: (3225, 110)\n",
      "The shape of y_test_onehot is: (299, 110)\n"
     ]
    }
   ],
   "source": [
    "#Convert to spectrum\n",
    "#68 - 178\n",
    "\n",
    "spe_len = 110\n",
    "y_train = np.rint(y_train)\n",
    "y_test = np.rint(y_test)\n",
    "\n",
    "y_train_onehot = np.zeros((y_train.shape[0], spe_len))\n",
    "y_test_onehot = np.zeros((y_test.shape[0], spe_len))\n",
    "\n",
    "y_train = y_train - 68\n",
    "y_test = y_test - 68\n",
    "\n",
    "for i in range(y_train.shape[0]):\n",
    "    y_train_onehot[i, int(y_train[i][0])] = 1\n",
    "    \n",
    "for i in range(y_test.shape[0]):\n",
    "    y_test_onehot[i, int(y_test[i][0])] = 1\n",
    "    \n",
    "print('The shape of y_train_onehot is: ' + str(y_train_onehot.shape))\n",
    "print('The shape of y_test_onehot is: ' + str(y_test_onehot.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xufanxiong/Desktop/pytorch/ori_model.py:96: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  model = Model(input=inputs, output=output)\n"
     ]
    }
   ],
   "source": [
    "model = ori_model((1000, 5), 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optm = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=3*10**(-4))\n",
    "model.compile(loss= \"categorical_crossentropy\", \n",
    "              optimizer=optm, \n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2580 samples, validate on 645 samples\n",
      "Epoch 1/100\n",
      "2580/2580 [==============================] - 77s 30ms/step - loss: 5.0598 - acc: 0.0233 - val_loss: 6.5718 - val_acc: 0.0109\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.01085, saving model to weight_troika_ori_2s.hdf5\n",
      "Epoch 2/100\n",
      "2580/2580 [==============================] - 71s 27ms/step - loss: 4.4252 - acc: 0.0539 - val_loss: 6.0910 - val_acc: 0.0124\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.01085 to 0.01240, saving model to weight_troika_ori_2s.hdf5\n",
      "Epoch 3/100\n",
      "2580/2580 [==============================] - 74s 29ms/step - loss: 3.9419 - acc: 0.1283 - val_loss: 7.1590 - val_acc: 0.0496\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.01240 to 0.04961, saving model to weight_troika_ori_2s.hdf5\n",
      "Epoch 4/100\n",
      "2580/2580 [==============================] - 77s 30ms/step - loss: 3.2865 - acc: 0.2562 - val_loss: 9.2718 - val_acc: 0.0295\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.04961\n",
      "Epoch 5/100\n",
      "2580/2580 [==============================] - 71s 28ms/step - loss: 2.5045 - acc: 0.4279 - val_loss: 10.4971 - val_acc: 0.0388\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.04961\n",
      "Epoch 6/100\n",
      " 768/2580 [=======>......................] - ETA: 46s - loss: 1.7881 - acc: 0.5859"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4c8af7d932ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                     verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filepath=\"weight_spe.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, \n",
    "                             monitor='val_acc', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "history = model.fit(X_train, y_train_onehot, \n",
    "                    epochs=100, \n",
    "                    batch_size=128, \n",
    "                    validation_split=0.2, \n",
    "                    callbacks=callbacks_list,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weight_spe.hdf5\"\n",
    "model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate\n",
    "eva = model.evaluate(X_test, y_test_onehot, batch_size=32, verbose=1, sample_weight=None)\n",
    "print()\n",
    "print(\"Loss = \" + str(eva[0]))\n",
    "print(\"Test Accuracy = \" + str(eva[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
