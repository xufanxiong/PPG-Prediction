{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xufanxiong/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pywt\n",
    "import timeit\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, preprocessing\n",
    "from scipy import signal\n",
    "from keras.layers import Dense, Flatten, Activation, TimeDistributed, Reshape\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import AveragePooling1D\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD, Adam, Nadam, Adamax\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras.callbacks\n",
    "from keras import initializers, regularizers\n",
    "from keras.layers.pooling import GlobalAvgPool2D\n",
    "from keras.layers.recurrent import LSTM, GRU, RNN\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU, ELU\n",
    "#from googlenet_custom_layers.py import LRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6132, 5, 1200)\n",
      "(6132, 5, 1200)\n",
      "(6132, 5, 1200, 2)\n",
      "(6132, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "signalTrain = np.zeros((6132, 5, 1200, 2))\n",
    "datasetTrain1 = sio.loadmat('Train_1_Correct_PPG1_TimeDist5.mat')\n",
    "datasetTrain2 = sio.loadmat('Train_1_Correct_PPG2_TimeDist5.mat')\n",
    "heart_rateTrain = datasetTrain1['Hr_train1']\n",
    "heart_rateTrain = (heart_rateTrain - 50) / 150\n",
    "\n",
    "signalTrain1 = datasetTrain1['Train1']\n",
    "signalTrain2 = datasetTrain2['Train2']\n",
    "signalTrain[:, :, :, 0] = signalTrain1;\n",
    "signalTrain[:, :, :, 1] = signalTrain2;\n",
    "\n",
    "print(signalTrain1.shape)\n",
    "print(signalTrain2.shape)\n",
    "print(signalTrain.shape)\n",
    "print(heart_rateTrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5518, 5, 1200, 2)\n",
      "(5518, 1)\n",
      "(614, 5, 1200, 2)\n",
      "(614, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_validate3, Y_train, Y_validate3 = train_test_split(signalTrain, heart_rateTrain, test_size=0.1, random_state=123)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_validate3.shape)\n",
    "print(Y_validate3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 5, 1200)\n",
      "(210, 5, 1200)\n",
      "(210, 5, 1200, 2)\n",
      "(210, 1)\n",
      "(210, 5, 1200)\n",
      "(210, 5, 1200)\n",
      "(210, 5, 1200, 2)\n",
      "(210, 1)\n",
      "(210, 5, 1200)\n",
      "(210, 5, 1200)\n",
      "(210, 5, 1200, 2)\n",
      "(210, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "signalTestA = np.zeros((210, 5, 1200, 2))\n",
    "datasetTest1 = sio.loadmat('Test_1_Correct_PPG1_TimeDist5.mat')\n",
    "datasetTest2 = sio.loadmat('Test_1_Correct_PPG2_TimeDist5.mat')\n",
    "heart_rateTestA = datasetTest1['Hr_testA1']\n",
    "heart_rateTestA = (heart_rateTestA - 50) / 150\n",
    "\n",
    "signalTest1A = datasetTest1['TestA1']\n",
    "signalTest2A = datasetTest2['TestA2']\n",
    "signalTestA[:, :, :, 0] = signalTest1A\n",
    "signalTestA[:, :, :, 1] = signalTest2A\n",
    "\n",
    "print(signalTest1A.shape)\n",
    "print(signalTest2A.shape)\n",
    "print(signalTestA.shape)\n",
    "print(heart_rateTestA.shape)\n",
    "\n",
    "\n",
    "signalTestB = np.zeros((210, 5, 1200, 2))\n",
    "heart_rateTestB = datasetTest1['Hr_testB1']\n",
    "heart_rateTestB = (heart_rateTestB - 50) / 150\n",
    "\n",
    "signalTest1B = datasetTest1['TestB1']\n",
    "signalTest2B = datasetTest2['TestB2']\n",
    "signalTestB[:, :, :, 0] = signalTest1B\n",
    "signalTestB[:, :, :, 1] = signalTest2B\n",
    "\n",
    "print(signalTest1B.shape)\n",
    "print(signalTest2B.shape)\n",
    "print(signalTestB.shape)\n",
    "print(heart_rateTestB.shape)\n",
    "\n",
    "\n",
    "signalTestC = np.zeros((210, 5, 1200, 2))\n",
    "heart_rateTestC = datasetTest1['Hr_testC1']\n",
    "heart_rateTestC = (heart_rateTestC - 50) / 150\n",
    "\n",
    "signalTest1C = datasetTest1['TestC1']\n",
    "signalTest2C = datasetTest2['TestC2']\n",
    "signalTestC[:, :, :, 0] = signalTest1C\n",
    "signalTestC[:, :, :, 1] = signalTest2C\n",
    "\n",
    "print(signalTest1C.shape)\n",
    "print(signalTest2C.shape)\n",
    "print(signalTestC.shape)\n",
    "print(heart_rateTestC.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 5, 1200, 2)\n",
      "(210, 1)\n"
     ]
    }
   ],
   "source": [
    "#Stable Data#\n",
    "X_testA, _, Y_testA, _ = train_test_split(signalTestA, heart_rateTestA, test_size=0, random_state=123)\n",
    "print(X_testA.shape)\n",
    "print(Y_testA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 5, 1200, 2)\n",
      "(210, 1)\n"
     ]
    }
   ],
   "source": [
    "#Stable Data#\n",
    "X_testB, _, Y_testB, _ = train_test_split(signalTestB, heart_rateTestB, test_size=0, random_state=123)\n",
    "print(X_testB.shape)\n",
    "print(Y_testB.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 5, 1200, 2)\n",
      "(210, 1)\n"
     ]
    }
   ],
   "source": [
    "#Stable Data#\n",
    "X_testC, _, Y_testC, _ = train_test_split(signalTestC, heart_rateTestC, test_size=0, random_state=123)\n",
    "print(X_testC.shape)\n",
    "print(Y_testC.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 5, 1200)\n",
      "(210, 5, 1200)\n",
      "(210, 5, 1200, 2)\n",
      "(210, 1)\n",
      "(210, 5, 1200)\n",
      "(210, 5, 1200)\n",
      "(210, 5, 1200, 2)\n",
      "(210, 1)\n",
      "(210, 5, 1200)\n",
      "(210, 5, 1200)\n",
      "(210, 5, 1200, 2)\n",
      "(210, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "signalTestD = np.zeros((210, 5, 1200, 2))\n",
    "datasetTest1 = sio.loadmat('Low_1_Correct_PPG1_TimeDist5.mat')\n",
    "datasetTest2 = sio.loadmat('Low_1_Correct_PPG2_TimeDist5.mat')\n",
    "heart_rateTestD = datasetTest1['Hr_testA1']\n",
    "heart_rateTestD = (heart_rateTestD - 50) / 150\n",
    "\n",
    "signalTest1D = datasetTest1['TestA1']\n",
    "signalTest2D = datasetTest2['TestA2']\n",
    "signalTestD[:, :, :, 0] = signalTest1D\n",
    "signalTestD[:, :, :, 1] = signalTest2D\n",
    "\n",
    "print(signalTest1D.shape)\n",
    "print(signalTest2D.shape)\n",
    "print(signalTestD.shape)\n",
    "print(heart_rateTestD.shape)\n",
    "\n",
    "\n",
    "signalTestE = np.zeros((210, 5, 1200, 2))\n",
    "heart_rateTestE = datasetTest1['Hr_testB1']\n",
    "heart_rateTestE = (heart_rateTestE - 50) / 150\n",
    "\n",
    "signalTest1E = datasetTest1['TestB1']\n",
    "signalTest2E = datasetTest2['TestB2']\n",
    "signalTestE[:, :, :, 0] = signalTest1E\n",
    "signalTestE[:, :, :, 1] = signalTest2E\n",
    "\n",
    "print(signalTest1E.shape)\n",
    "print(signalTest2E.shape)\n",
    "print(signalTestE.shape)\n",
    "print(heart_rateTestE.shape)\n",
    "\n",
    "\n",
    "signalTestF = np.zeros((210, 5, 1200, 2))\n",
    "heart_rateTestF = datasetTest1['Hr_testC1']\n",
    "heart_rateTestF = (heart_rateTestF - 50) / 150\n",
    "\n",
    "signalTest1F = datasetTest1['TestC1']\n",
    "signalTest2F = datasetTest2['TestC2']\n",
    "signalTestF[:, :, :, 0] = signalTest1F\n",
    "signalTestF[:, :, :, 1] = signalTest2F\n",
    "\n",
    "print(signalTest1F.shape)\n",
    "print(signalTest2F.shape)\n",
    "print(signalTestF.shape)\n",
    "print(heart_rateTestF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 5, 1200, 2)\n",
      "(210, 1)\n"
     ]
    }
   ],
   "source": [
    "#Stable Data#\n",
    "X_testD, _, Y_testD, _ = train_test_split(signalTestD, heart_rateTestD, test_size=0, random_state=123)\n",
    "print(X_testD.shape)\n",
    "print(Y_testD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 5, 1200, 2)\n",
      "(210, 1)\n"
     ]
    }
   ],
   "source": [
    "#Stable Data#\n",
    "X_testE, _, Y_testE, _ = train_test_split(signalTestE, heart_rateTestE, test_size=0, random_state=123)\n",
    "print(X_testE.shape)\n",
    "print(Y_testE.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 5, 1200, 2)\n",
      "(210, 1)\n"
     ]
    }
   ],
   "source": [
    "#Stable Data#\n",
    "X_testF, _, Y_testF, _ = train_test_split(signalTestF, heart_rateTestF, test_size=0, random_state=123)\n",
    "print(X_testF.shape)\n",
    "print(Y_testF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 5, 1200)\n",
      "(210, 5, 1200)\n",
      "(210, 5, 1200, 2)\n",
      "(210, 1)\n",
      "(210, 5, 1200)\n",
      "(210, 5, 1200)\n",
      "(210, 5, 1200, 2)\n",
      "(210, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "signalTestG = np.zeros((210, 5, 1200, 2))\n",
    "datasetTest1 = sio.loadmat('Validation_1_Correct_PPG1_TimeDist5.mat')\n",
    "datasetTest2 = sio.loadmat('Validation_1_Correct_PPG2_TimeDist5.mat')\n",
    "heart_rateTestG = datasetTest1['Hr_validationA1']\n",
    "heart_rateTestG = (heart_rateTestG - 50) / 150\n",
    "\n",
    "signalTest1G = datasetTest1['ValidationA1']\n",
    "signalTest2G = datasetTest1['ValidationA2']\n",
    "signalTestG[:, :, :, 0] = signalTest1G\n",
    "signalTestG[:, :, :, 1] = signalTest2G\n",
    "\n",
    "print(signalTest1G.shape)\n",
    "print(signalTest2G.shape)\n",
    "print(signalTestG.shape)\n",
    "print(heart_rateTestG.shape)\n",
    "\n",
    "\n",
    "signalTestH = np.zeros((210, 5, 1200, 2))\n",
    "heart_rateTestH = datasetTest2['Hr_validationB1']\n",
    "heart_rateTestH = (heart_rateTestH - 50) / 150\n",
    "\n",
    "signalTest1H = datasetTest2['ValidationB1']\n",
    "signalTest2H = datasetTest2['ValidationB2']\n",
    "signalTestH[:, :, :, 0] = signalTest1H\n",
    "signalTestH[:, :, :, 1] = signalTest2H\n",
    "\n",
    "print(signalTest1H.shape)\n",
    "print(signalTest2H.shape)\n",
    "print(signalTestH.shape)\n",
    "print(heart_rateTestH.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 5, 1200, 2)\n",
      "(210, 1)\n",
      "(210, 5, 1200, 2)\n",
      "(210, 1)\n"
     ]
    }
   ],
   "source": [
    "X_testG, _, Y_testG, _ = train_test_split(signalTestG, heart_rateTestG, test_size=0, random_state=123)\n",
    "print(X_testG.shape)\n",
    "print(Y_testG.shape)\n",
    "\n",
    "X_testH, _, Y_testH, _ = train_test_split(signalTestH, heart_rateTestH, test_size=0, random_state=123)\n",
    "print(X_testH.shape)\n",
    "print(Y_testH.shape)\n",
    "\n",
    "X_validate = np.zeros((1034, 5, 1200, 2))\n",
    "X_validate  [0:210, :, :, :] = X_testG\n",
    "X_validate[210:420, :, :, :] = X_testH\n",
    "X_validate   [420:, :, :, :] = X_validate3\n",
    "\n",
    "Y_validate = np.zeros((1034, 1))\n",
    "Y_validate  [0:210, :] = Y_testG\n",
    "Y_validate[210:420, :] = Y_testH\n",
    "Y_validate   [420:, :] = Y_validate3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The first layer in a Sequential model must get an `input_shape` or `batch_input_shape` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c2fe81216f25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'glorot_uniform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0;31m# know about its input shape. Otherwise, that's an error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch_input_shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                         raise ValueError('The first layer in a '\n\u001b[0m\u001b[1;32m    485\u001b[0m                                          \u001b[0;34m'Sequential model must '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                                          \u001b[0;34m'get an `input_shape` or '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The first layer in a Sequential model must get an `input_shape` or `batch_input_shape` argument."
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(TimeDistributed(Conv1D(64, 16, padding='same', input_shape=(1200, 2), kernel_initializer='glorot_uniform')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "# second part\n",
    "model.add(TimeDistributed(Conv1D(64, 16, padding='same')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(Dropout(0.30)))\n",
    "model.add(TimeDistributed(Conv1D(64, 16, padding='same')))\n",
    "model.add(TimeDistributed(MaxPooling1D()))\n",
    "# third layers\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(Dropout(0.30)))\n",
    "model.add(TimeDistributed(Conv1D(64, 16, padding='same')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(Dropout(0.30)))\n",
    "model.add(TimeDistributed(Conv1D(64, 16, padding='same')))\n",
    "model.add(TimeDistributed(MaxPooling1D()))\n",
    "\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(Dropout(0.30)))\n",
    "model.add(TimeDistributed(Conv1D(128, 16, padding='same', kernel_initializer='glorot_uniform')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(Dropout(0.30)))\n",
    "model.add(TimeDistributed(Conv1D(128, 16, padding='same')))\n",
    "model.add(TimeDistributed(MaxPooling1D()))\n",
    "\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(Dropout(0.30)))\n",
    "model.add(TimeDistributed(Conv1D(128, 16, padding='same')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(Dropout(0.30)))\n",
    "model.add(TimeDistributed(Conv1D(128, 16, padding='same')))\n",
    "model.add(TimeDistributed(MaxPooling1D()))\n",
    "\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(Dropout(0.30)))\n",
    "model.add(TimeDistributed(Conv1D(192, 16, padding='same', kernel_initializer='glorot_uniform')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(Dropout(0.30)))\n",
    "model.add(TimeDistributed(Conv1D(192, 16, padding='same')))\n",
    "model.add(TimeDistributed(MaxPooling1D()))\n",
    "\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(Dropout(0.30)))\n",
    "model.add(TimeDistributed(Conv1D(192, 16, padding='same')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(Dropout(0.30)))\n",
    "model.add(TimeDistributed(Conv1D(192, 16, padding='same')))\n",
    "model.add(TimeDistributed(MaxPooling1D()))\n",
    "\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(Dropout(0.30)))\n",
    "model.add(TimeDistributed(Conv1D(256, 16, padding='same', kernel_initializer='glorot_uniform')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(Dropout(0.30)))\n",
    "model.add(TimeDistributed(Conv1D(256, 16, padding='same')))\n",
    "model.add(TimeDistributed(MaxPooling1D()))\n",
    "\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(Dropout(0.30)))\n",
    "model.add(TimeDistributed(Conv1D(256, 16, padding='same')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(Dropout(0.30)))\n",
    "model.add(TimeDistributed(Conv1D(256, 16, padding='same')))\n",
    "model.add(TimeDistributed(MaxPooling1D()))\n",
    "##\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(TimeDistributed(Dense(units=100, activation='sigmoid')))\n",
    "model.add(LSTM(units=256, return_sequences=True))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(units=64, return_sequences=False))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units=1, activation='sigmoid', kernel_initializer='he_uniform', bias_initializer='zeros'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization setup\n",
    "optm = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=3*10**(-4))\n",
    "sgd = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)\n",
    "model.compile(\n",
    "    loss='mae',\n",
    "    optimizer=optm,\n",
    "    metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights.bestTimeDist3best3.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5518 samples, validate on 1034 samples\n",
      "Epoch 1/50\n",
      " - 33s - loss: 0.0811 - mean_squared_error: 0.0123 - val_loss: 0.0603 - val_mean_squared_error: 0.0088\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06031, saving model to weights.bestTimeDist3best3.hdf5\n",
      "Epoch 2/50\n",
      " - 25s - loss: 0.0597 - mean_squared_error: 0.0072 - val_loss: 0.0775 - val_mean_squared_error: 0.0117\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.06031\n",
      "Epoch 3/50\n",
      " - 25s - loss: 0.0503 - mean_squared_error: 0.0056 - val_loss: 0.0828 - val_mean_squared_error: 0.0123\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.06031\n",
      "Epoch 4/50\n",
      " - 26s - loss: 0.0413 - mean_squared_error: 0.0038 - val_loss: 0.0571 - val_mean_squared_error: 0.0067\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.06031 to 0.05712, saving model to weights.bestTimeDist3best3.hdf5\n",
      "Epoch 5/50\n",
      " - 25s - loss: 0.0333 - mean_squared_error: 0.0026 - val_loss: 0.0442 - val_mean_squared_error: 0.0040\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.05712 to 0.04416, saving model to weights.bestTimeDist3best3.hdf5\n",
      "Epoch 6/50\n",
      " - 25s - loss: 0.0325 - mean_squared_error: 0.0025 - val_loss: 0.0447 - val_mean_squared_error: 0.0045\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.04416\n",
      "Epoch 7/50\n",
      " - 26s - loss: 0.0307 - mean_squared_error: 0.0024 - val_loss: 0.0458 - val_mean_squared_error: 0.0039\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.04416\n",
      "Epoch 8/50\n",
      " - 26s - loss: 0.0294 - mean_squared_error: 0.0022 - val_loss: 0.0483 - val_mean_squared_error: 0.0046\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.04416\n",
      "Epoch 9/50\n",
      " - 26s - loss: 0.0279 - mean_squared_error: 0.0020 - val_loss: 0.0456 - val_mean_squared_error: 0.0050\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.04416\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-07d877f484c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                     verbose=2)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    211\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[1;32m    212\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                              verbose=0)\n\u001b[0m\u001b[1;32m    214\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                             \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "history = model.fit(X_train, \n",
    "                    Y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=50,\n",
    "#                    validation_split=0.2,\n",
    "                    validation_data = (X_validate, Y_validate),\n",
    "                    shuffle=True,\n",
    "                    callbacks=callbacks_list,\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optm = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=3*10**(-4))\n",
    "sgd = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)\n",
    "model.compile(\n",
    "    loss='mae',\n",
    "    optimizer=optm,\n",
    "    metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the best model\n",
    "model.load_weights(\"weights.bestTimeDist3best3.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "history = model.fit(X_train, \n",
    "                    Y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=100,\n",
    "#                    validation_split=0.2,\n",
    "                    validation_data = (X_validate, Y_validate),\n",
    "                    shuffle=True,\n",
    "                    callbacks=callbacks_list,\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimization setup\n",
    "optm = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=3*10**(-4))\n",
    "sgd = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)\n",
    "model.compile(\n",
    "    loss='mae',\n",
    "    optimizer=optm,\n",
    "    metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using the best model\n",
    "model.load_weights(\"weights.bestTimeDist3best3.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "history = model.fit(X_train, \n",
    "                    Y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=100,\n",
    "#                    validation_split=0.2,\n",
    "                    validation_data = (X_validate, Y_validate),\n",
    "                    shuffle=True,\n",
    "                    callbacks=callbacks_list,\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "fig.savefig('performance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_testA)\n",
    "Y_testA = Y_testA * 150 + 50\n",
    "Y_pred = Y_pred * 150 + 50\n",
    "plt.figure\n",
    "plt.scatter(Y_testA, Y_pred)\n",
    "plt.plot((75,95), (75,95))\n",
    "Y_error = abs(Y_pred - Y_testA)\n",
    "print('The avarage predict error: ', np.mean(Y_error))\n",
    "print('The mean squared error:', metrics.mean_squared_error(Y_testA, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_testB)\n",
    "Y_testB = Y_testB * 150 + 50\n",
    "Y_pred = Y_pred * 150 + 50\n",
    "plt.figure\n",
    "plt.scatter(Y_testB, Y_pred)\n",
    "plt.plot((85,110), (85, 110))\n",
    "Y_error = abs(Y_pred - Y_testB)\n",
    "print('The avarage predict error: ', np.mean(Y_error))\n",
    "print('The mean squared error:', metrics.mean_squared_error(Y_testB, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_testC)\n",
    "Y_testC = Y_testC * 150 + 50\n",
    "Y_pred = Y_pred * 150 + 50\n",
    "plt.figure\n",
    "plt.scatter(Y_testC, Y_pred)\n",
    "plt.plot((80, 100), (80, 100))\n",
    "Y_error = abs(Y_pred - Y_testC)\n",
    "print('The avarage predict error: ', np.mean(Y_error))\n",
    "print('The mean squared error:', metrics.mean_squared_error(Y_testC, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_testD)\n",
    "Y_testD = Y_testD * 150 + 50\n",
    "Y_pred = Y_pred * 150 + 50\n",
    "plt.figure\n",
    "plt.scatter(Y_testD, Y_pred)\n",
    "plt.plot((75,100), (75,100))\n",
    "Y_error = abs(Y_pred - Y_testD)\n",
    "print('The avarage predict error: ', np.mean(Y_error))\n",
    "print('The mean squared error:', metrics.mean_squared_error(Y_testD, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_testE)\n",
    "Y_testE = Y_testE * 150 + 50\n",
    "Y_pred = Y_pred * 150 + 50\n",
    "plt.figure\n",
    "plt.scatter(Y_testE, Y_pred)\n",
    "plt.plot((75,105), (75,105))\n",
    "Y_error = abs(Y_pred - Y_testE)\n",
    "print('The avarage predict error: ', np.mean(Y_error))\n",
    "print('The mean squared error:', metrics.mean_squared_error(Y_testE, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_testF)\n",
    "Y_testF = Y_testF * 150 + 50\n",
    "Y_pred = Y_pred * 150 + 50\n",
    "plt.figure\n",
    "plt.scatter(Y_testF, Y_pred)\n",
    "plt.plot((80, 105), (80, 105))\n",
    "Y_error = abs(Y_pred - Y_testF)\n",
    "print('The avarage predict error: ', np.mean(Y_error))\n",
    "print('The mean squared error:', metrics.mean_squared_error(Y_testF, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_train)\n",
    "Y_train = Y_train * 150 + 50\n",
    "Y_pred = Y_pred * 150 + 50\n",
    "plt.figure\n",
    "plt.scatter(Y_train, Y_pred)\n",
    "plt.plot((60,170), (60,170))\n",
    "Y_error = abs(Y_pred - Y_train)\n",
    "print('The avarage predict error: ', np.mean(Y_error))\n",
    "print('The mean squared error:', metrics.mean_squared_error(Y_train, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the best model\n",
    "model.load_weights(\"weights.bestTimeDist3best3.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_testA)\n",
    "Y_pred = Y_pred * 150 + 50\n",
    "plt.figure\n",
    "plt.scatter(Y_testA, Y_pred)\n",
    "plt.plot((75, 95), (75, 95))\n",
    "Y_error = abs(Y_pred - Y_testA)\n",
    "print('The avarage predict error: ', np.mean(Y_error))\n",
    "print('The mean squared error:', metrics.mean_squared_error(Y_testA, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_testB)\n",
    "Y_pred = Y_pred * 150 + 50\n",
    "plt.figure\n",
    "plt.scatter(Y_testB, Y_pred)\n",
    "plt.plot((85,110), (85,110))\n",
    "Y_error = abs(Y_pred - Y_testB)\n",
    "print('The avarage predict error: ', np.mean(Y_error))\n",
    "print('The mean squared error:', metrics.mean_squared_error(Y_testB, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_testC)\n",
    "Y_pred = Y_pred * 150 + 50\n",
    "plt.figure\n",
    "plt.scatter(Y_testC, Y_pred)\n",
    "plt.plot((80,105), (80,105))\n",
    "Y_error = abs(Y_pred - Y_testC)\n",
    "print('The avarage predict error: ', np.mean(Y_error))\n",
    "print('The mean squared error:', metrics.mean_squared_error(Y_testC, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_testD)\n",
    "Y_pred = Y_pred * 150 + 50\n",
    "plt.figure\n",
    "plt.scatter(Y_testD, Y_pred)\n",
    "plt.plot((75, 100), (75, 100))\n",
    "Y_error = abs(Y_pred - Y_testD)\n",
    "print('The avarage predict error: ', np.mean(Y_error))\n",
    "print('The mean squared error:', metrics.mean_squared_error(Y_testD, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_testE)\n",
    "Y_pred = Y_pred * 150 + 50\n",
    "plt.figure\n",
    "plt.scatter(Y_testE, Y_pred)\n",
    "plt.plot((75,105), (75,105))\n",
    "Y_error = abs(Y_pred - Y_testE)\n",
    "print('The avarage predict error: ', np.mean(Y_error))\n",
    "print('The mean squared error:', metrics.mean_squared_error(Y_testE, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_testF)\n",
    "Y_pred = Y_pred * 150 + 50\n",
    "plt.figure\n",
    "plt.scatter(Y_testF, Y_pred)\n",
    "plt.plot((80, 105), (80, 105))\n",
    "Y_error = abs(Y_pred - Y_testF)\n",
    "print('The avarage predict error: ', np.mean(Y_error))\n",
    "print('The mean squared error:', metrics.mean_squared_error(Y_testF, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_train)\n",
    "#Y_train = Y_train * 150 + 50\n",
    "Y_pred = Y_pred * 150 + 50\n",
    "plt.figure\n",
    "plt.scatter(Y_train, Y_pred)\n",
    "plt.plot((60,170), (60,170))\n",
    "Y_error = abs(Y_pred - Y_train)\n",
    "print('The avarage predict error: ', np.mean(Y_error))\n",
    "print('The mean squared error:', metrics.mean_squared_error(Y_train, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
